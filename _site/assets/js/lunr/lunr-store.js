var store = [{
        "title": "SQL Scavenger Hunt with BigQuery",
        "excerpt":"What I Learned  How to SELECT columns FROM a table WHERE certain conditions are met.  How to use GROUP BY to apply functions like COUNT() to each group separately. And how to filter out groups using HAVING.  How to return sorted data with ORDER BY and how to work with dates in SQL.  How to use WITH and AS to make your code succinct and easy to read.  How use use JOIN to align data from multiple tables.                                                                                                                                                                                            Open Dataset from Google  My Shared Kernels  SQL Scavenger Hunt: Day 1  SQL Scavenger Hunt: Day 2  SQL Scavenger Hunt: Day 3  SQL Scavenger Hunt: Day 4  SQL Scavenger Hunt: Day 5What is Next?Try writing an interesting analysis kernel on one of the BigQuery datasets and making it public. (The winners will win $1000 on on February 15th and 22nd.)","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/SQL-scavenger-hunt/",
        "teaser":"http://www.congruentsolutions.com/wp-content/uploads/2016/03/Google-BigQuery-300x285.png"},{
        "title": "Program Structures and Algorithms",
        "excerpt":"What I Learned  fundamentals of programming data structures and basic algorithms  common data structures          arrays, linked list, stacks and queues, hash tables and hash maps      trees, graphs, suffix trees      other specialized data structures        order of complexities for each one of these data structures  searching and sorting  backtracking  dynamic programming  bit manipulation  pattern searching‚Ä¶Course Links  Repository  Code Lab GPA: 4.0/4.0 üíØ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/algorithm/",
        "teaser":"http://localhost:4000/assets/images/teasers/info6205.png"},{
        "title": "Movie Rating and Recommendation Data Pipeline",
        "excerpt":"What I LearnedData ingestion using kafka; Use machine learning to predict the movie rating  Week 1: Produce data to kafka [done]Send movie title data [done]  Send rating data simulating the streaming [done]  Week 2: Add error/exception handling and more comments in the source code [done]  Week 3: Study the classification algorithm, and apply to Netflix dataset  Week 4: Adjust the ML model &amp; Summarize this projectProject Homepage","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/data-pipeline/",
        "teaser":"https://s3-us-west-2.amazonaws.com/github-photo-links/Screen+Shot+2018-01-26+at+4.32.24+PM.png"},{
        "title": "Application Development and Engineering",
        "excerpt":"What I Learned  Procedural and object-oriented paradigms (in Java)  Standard data types, primitive classes, wrapper classes  OO: encapsulation, inheritance, polymorphism, and abstraction  Abstract classes, interfaces and inner classes  Exceptions and multi-threaded programming  Collection framework, Array, Collections, List, Set, and Map  File IO and exceptions handling  Building user interfaces using Java Swing                                                                                                                                                                                            Screenshots of course projects.  Course Links  Homepage  Repository  Projects          Hang Man      Game of Hearts      Inventory Management      GPA: 4.0/4.0 üíØ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/programming/",
        "teaser":"http://localhost:4000/assets/images/application-development/inventory/inventory-ist.png"},{
        "title": "About me and this website",
        "excerpt":"My ProfessionI am studying Information Management at Northeastern University in Seattle from 2017 to 2019. Before, I was a software engineer in Oracle, and then a tech lead in Sony for 9+ years in total.My prospective profession is to be a data engineer/scientist after graduation in May 2019. So I will show some of my school/side projects on this website, including some technical articles related to java, python, web-design, database, cloud computing, big data and data science.If you would like to know more about my profession, please check my cv.My InterestsMindfulness and productivity are my most interesting subject, and I am still on my way to practice, and then build my own methodology.My other hobbies are reading, travel, and photography. I will keep writing notes and articles (mostly in Chinese) on my life blog.","categories": [],
        "tags": [],
        "url": "http://localhost:4000/mindfulness-site-launched/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Start small, achieve BIG!",
        "excerpt":"Many people already know this, but why I mention it again, simply because it is very important and easy to forget.Nowadays, people just want to accomplish big and fancy things and tend to ignore the small ones. However, the universal law cannot be ignored that big achievements start from small.  Finishing a marathon starts with a step of running and then the next step.Writing a blog (and do not stop) is a big thing but it starts with a simple post. As to me, maybe it starts a feature of a programming language or a simple linear regression model. By keeping learning and practicing days after days, eventually, I will achieve big.2018-01-17, Seattle","categories": [],
        "tags": [],
        "url": "http://localhost:4000/start-small-achieve-big/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Use `intertools` to solve combination and permutation problems in leetcode",
        "excerpt":"In python, package itertools  is a very powerful and efficient looping tool. Here are some examples of how to solve some leetcode algorithm problems using this tool.Quick Look at Combinatoric Iterators            Iterator      Arguments      Results                  product()      p, q, ‚Ä¶ [repeat=1]      cartesian product, equivalent to a nested for-loop              permutations()      p[, r]      r-length tuples, all possible orderings, no repeated elements              combinations()      p, r      r-length tuples, in sorted order, no repeated elements              combinations_with_replacement()      p, r      r-length tuples, in sorted order, with repeated elements              product('ABCD', repeat=2)      ¬†      AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DD              permutations('ABCD', 2)      ¬†      AB AC AD BA BC BD CA CB CD DA DB DC              combinations('ABCD', 2)      ¬†      AB AC AD BC BD CD              combinations_with_replacement('ABCD', 2)      ¬†      AA AB AC AD BB BC BD CC CD DD      LeetCode Problems and SolutionsCombinationsDescriptionGiven two integers n and k, return all possible combinations of k numbers out of 1 ‚Ä¶ n.For example,If n = 4 and k = 2, a solution is:[  [2,4],  [3,4],  [2,3],  [1,2],  [1,3],  [1,4],]SolutionIt likes talored for the itertools.combinations() function. :laughing:import itertoolsclass Solution:    def combine(self, n, k):        \"\"\"        :type n: int        :type k: int        :rtype: List[List[int]]        \"\"\"        nums = [x for x in range(1, n+1)]        return list(itertools.combinations(nums, k))Combination Sum IIIDescriptionFind all possible combinations of *k* numbers that add up to a number *n*, given that only numbers from 1 to 9 can be used and each combination should be a unique set of numbers.*Example 1:*Input:  k = 3,  n = 7Output:[[1,2,4]]*Example 2:*Input:  k = 3,  n = 9Output:[[1,2,6], [1,3,5], [2,3,4]]Solutionimport itertoolsclass Solution:    def combinationSum3(self, k, n):        \"\"\"        :type k: int        :type n: int        :rtype: List[List[int]]        \"\"\"        if n &lt; 1 or n &gt; (1 + 9) * 9 / 2: return [] # n is too small or big        nums = [x for x in range(1, 10)] # form a list of numbers from 1 to 10 (exclusive)        return [e for e in itertools.combinations(nums, k) if sum(e) == n] # comprehensionNotereturn list(e for e in itertools.combinations(nums, k) if sum(e) == n) will also work but it is a little bit slow that the list comprehension which is used in the provided solution.PermutationsDescriptionGiven a collection of distinct numbers, return all possible permutations.For example,[1,2,3] have the following permutations:[  [1,2,3],  [1,3,2],  [2,1,3],  [2,3,1],  [3,1,2],  [3,2,1]]Solutionimport itertoolsclass Solution:    def permute(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        return list(itertools.permutations(nums))Permutations IIDescriptionGiven a collection of numbers that might contain duplicates, return all possible unique permutations.For example,[1,1,2] have the following unique permutations:[  [1,1,2],  [1,2,1],  [2,1,1]]Solutionimport itertoolsclass Solution:    def permuteUnique(self, nums):        \"\"\"        :type nums: List[int]        :rtype: List[List[int]]        \"\"\"        return list(set(itertools.permutations(nums))) # get rid of duplicatesLimitationThe built-in function is very handy and cool. However, it is always slow to search all the combinations/permutations in a list of numbers. That‚Äôs where other algorithms like backtracking come in because it can prune the unnecessary branches when searching. Later, I will introduce the solution to these questions as well.Combination SumCombination Sum IICombination Sum IV2018-01-18, Seattle","categories": [],
        "tags": [],
        "url": "http://localhost:4000/itertools-in-leetcode/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Get timestamp in millisecond in python 2",
        "excerpt":"Just a quick code snippet to get the millisecond in python.# python 2.ximport timeimport calendar# to get the time stamp in 1. millisecond (Linux stype from the epoch 1970-01-01 00:00:00)dt = time.strptime('1970-01-02', '%Y-%m-%d')print(dt)ts = calendar.timegm(dt)print(ts * 1000)2018-01-21, Seattle","categories": [],
        "tags": [],
        "url": "http://localhost:4000/get-timestamp-python/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "My Lynda Certificates!",
        "excerpt":"Utilize Online Learning Resource: LyndaThe university offered this great resource for students to learn online. Why not make use of it?! The overall quality of Lynda courses is great, and I can also earn some certificates and put it in linkedin.Here is some sample certificates, and click to check all my Lynda certificates. This is a small start but it will become big days after days.2018-01-21, Seattle","categories": ["learn"],
        "tags": ["certificate"],
        "url": "http://localhost:4000/learn/lynda-certificates/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Got new skills: Spark & Scala",
        "excerpt":"As part of my knowledge base in Big Data, I completed two new certificates in Spark and Scala. My next step is to practice Kafka and Spark in two real projects, and got familiar with the APIs.2018-01-29, Seattle","categories": ["learn"],
        "tags": ["certificate"],
        "url": "http://localhost:4000/learn/spark-scala/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Understand Idempotence of Reduce Function in MongoDB",
        "excerpt":"Here are the characteristics of Reduce Function Idempotence:  a map-reduce operation may call a reduce multiple times for the same key  it won‚Äôt call a reduce for single instances of a key in the working set  the reduce function must return a value of the same type as the value emitted from the map function.Problem: Calculate Average Price Using MapReduceDescriptionFind the average price of stock_price_high values for each stock and the total average price for all the stock using map-reduce in MongoDB.DatasetThe dataset comes from http://msis.neu.edu/nyse, and it has more than 9 million stock price records. After importing them to MongoDB database using mongdoimport command, the data format is like below.&gt; use nysedb;&gt; db.stocks.count()9211031&gt; db.stocks.findOne();{    \"_id\" : ObjectId(\"5a7158a80cf8a8197ba29570\"),    \"exchange\" : \"NYSE\",    \"stock_symbol\" : \"AEA\",    \"date\" : \"2010-02-08\",    \"stock_price_open\" : 4.42,    \"stock_price_high\" : 4.42,    \"stock_price_low\" : 4.21,    \"stock_price_close\" : 4.24,    \"stock_volume\" : 205500,    \"stock_price_adj_close\" : 4.24}Note: The collection name is ‚Äòstocks‚Äô. And here is the bash script to import this dataset.!/bin/bashFILES=NYSE/NYSE_daily_prices_*.csvfor f in $FILESdo    echo \"Processing $f file...\"    # ls -l $f    mongoimport --db nysedb --collection stocks --type csv --headerline --file $fdoneSub-Problem: Calculate the Average Price for Each StockLet‚Äôs take the stock_price_highproperty as an example to calculate the average.Approach I:  MapReduce using forEach()let map = function () {    emit(this.stock_symbol, this.stock_price_high);}let reduce = function (key, values) {  sum = 0;  num = 0;    values.forEach(function (v) {    sum += v;    num += 1;  });    if (num &gt; 0){    return sum / num;  } else {    return 0;  };}db.stocks.mapReduce(map, reduce, {out:\"mr_stock_price_avg_each\"})This job runs successfully with 9211031 inputs and 2853 outputs. And here is the result of ‚ÄúAA‚Äù.  { ‚Äú_id‚Äù : ‚ÄúAA‚Äù, ‚Äúvalue‚Äù : 64.26173484209001 }The function looks quite straightforward. Now let‚Äôs use another short version and cross-validated.Approach II: MapReduce using Array.sum()let map = function () {    emit(this.stock_symbol, this.stock_price_high);}let reduce = function (key, values) {  sum = Array.sum(values);  num = values.length;}db.stocks.mapReduce(map, reduce, {out:\"mr_stock_price_avg_each\"})Again, here is the result of ‚ÄúAA‚Äù.  { ‚Äú_id‚Äù : ‚ÄúAA‚Äù, ‚Äúvalue‚Äù : 64.26173484209001 }Validation using NoSQL AggregateThe value seems matching the previous approach. How could it be wrong? However, you will be surprised when you run this NoSQL aggregate to calculate the same average value for stock ‚ÄúAA‚Äù.&gt; db.stocks.aggregate(   [     {       $match: {stock_symbol: \"AA\"}     },     {       $group:         {           _id: \"$stock_symbol\",           avgAmount: { $avg: \"$stock_price_high\" }         }     }   ])However, the output is 52.45968205467008, which is different. Now, which one is right?Since the stock ‚ÄúAA‚Äù‚Äôs records are all in file NYSE_daily_prices_A.csv, use Excel can easily calculate the average, which is also 52.45968205. So it must be the right value. (Consider they are 52.45968205 since precision after the 7th of . does not matter much in this case.)Explanation and FixHere is the code to fix the bug.let map = function () {    emit(this.stock_symbol, {\"price\":this.stock_price_high, \"count\":1});}let reduce = function (key, values) {  reducedVal = {price: 0, count: 0}    values.forEach(function(v){    reducedVal.price += v.price;    reducedVal.count += v.count;  });    return reducedVal;}let average = function (key, reducedValue) {  return reducedValue.price / reducedValue.count;}db.stocks.mapReduce(map, reduce, {out:\"mr_stock_price_avg_each_fix\", finalize:average})But WHY?Go back and check Requirements for the reduce Function, notice this requirement:  Because it is possible to invoke the reduce function more than once for the same key, the following properties need to be true:            the type of the return object must be identical to the type of the value emitted by the mapfunction.              the reduce function must be associative. The following statement must be true:      reduce(key, [ C, reduce(key, [ A, B ]) ] ) == reduce( key, [ C, A, B ] )However, average operation is not associative. so it could not be in the reduce method.For example, considering stock ‚ÄúAA‚Äù with prices [1, 2, 6], the correct average should be 3. However, if the reduce job somehow is called twice, the first of which has input [1, 2], then the average is 1.5. Then the second call of [1.5, 6] gets 3.75 as the average, which is wrong. (Number 1.5 is the result of first reduce output.) It looks like below if you put it in the formula.  average(‚ÄúAA‚Äù, [6, average (‚ÄúAA‚Äù, [1, 2])]) ‚â† average (‚ÄúAA‚Äù, [6, 1, 2])So, it has to use a finalize method, which manipulates the result of the reduce job after it finishes.Sub-Problem: Calculate the Average Price for All StocksApproachReuse and modified the solution of the previous sub-problem. Here is the algorithm.  Have global variable sumAll to store the running sum for every occurrence of a stock  Counter the number of prices, and store in the countAll  Return a list [avgEach, avgAll] in the finalizerlet map = function () {    emit(this.stock_symbol, {\"price\":this.stock_price_high, \"count\":1});}let reduce = function (key, values) {  reducedVal = {price: 0, count: 0}    values.forEach(function(v){    reducedVal.price += v.price;    reducedVal.count += v.count;        sumAll += v.price;    countAll += v.count;  });    return reducedVal;}let avgAll = function (key, reducedValue) {  return { avg: reducedValue.price / reducedValue.count, allAvg: sumAll/countAll }}db.stocks.mapReduce(map, reduce, {out:\"mr_stock_price_avg_all\", scope:{sumAll:0, countAll:0},finalize:avgAll})Let‚Äôs check the output value of ‚ÄúAA‚Äù:{ \"_id\" : \"AA\", \"value\" : { \"avg\" : 52.459682054670246, \"allAvg\" : 32.96151152346334 } }Then validate this result using aggregation in mongo shell.&gt; db.stocks.aggregate(   [     {       $group:{         _id: null,         averageHighPrice: { $avg: \"$stock_price_high\" }       }     }   ]){ \"_id\" : null, \"averageHighPrice\" : 29.0213587773182 }What? Mismatch again!But WHY?Because the MongoDB engine won‚Äôt call a reduce for single instances of a key in the working set.To fix this problem, simply calculate the running sum and counts in the map function, because every record will be mapped no matter what.So here is the corrected solution.let map = function () {    sumAll += this.stock_price_high;    countAll += 1    emit(this.stock_symbol, {\"price\":this.stock_price_high, \"count\":1});}let reduce = function (key, values) {  reducedVal = {price: 0, count: 0}    values.forEach(function(v){    reducedVal.price += v.price;    reducedVal.count += v.count;  });    return reducedVal;}let avgAll = function (key, reducedValue) {  return { avg: reducedValue.price / reducedValue.count, allAvg: sumAll/countAll }}db.stocks.mapReduce(map, reduce, {out:\"mr_stock_price_avg_all\", scope:{sumAll:0, countAll:0},finalize:avgAll})SummaryWhen using the map-reduce, it is essential to follow the requirement of reduce function as below. Any violation may lead to incorrect result.  MongoDB will not call the reduce function for a key that has only a single value. The valuesargument is an array whose elements are the value objects that are ‚Äúmapped‚Äù to the key.  MongoDB can invoke the reduce function more than once for the same key. In this case, the previous output from the reduce function for that key will become one of the input values to the next reduce function invocation for that key.  The reduce function can access the variables defined in the scope parameter.","categories": ["database"],
        "tags": ["mongodb"],
        "url": "http://localhost:4000/database/mongodb-mapreduce/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Troubleshoot HDFS Connection Exception",
        "excerpt":"Error Messageubuntu@ip-172-31-43-69:~/apps/hadoop$ hadoop fs -ls /ls: Call From ip-172-31-43-69.us-west-2.compute.internal/172.31.43.69 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefusedExpectedhadoop fs -ls / can list all the folders and files under / in the HDFSContextIt happens every time after rebooting EC2 hosting Hadoop 3.0.0WorkaroundFormat the filesystem:$ bin/hdfs namenode -formatStart NameNode daemon and DataNode daemon:$ sbin/start-dfs.shThe hadoop daemon log output is written to the HADOOP_LOG_DIR directory (defaults to $HADOOP_HOME/logs)Browse the web interface for the NameNode; by default it is available at:NameNode - http://localhost:50070/Make the HDFS directories required to execute MapReduce jobs:$hadoop fs -mkdir /user$hadoop fs -mkdir /user/[username]Note: the [username] is should be replaced by actual name like ‚Äòubuntu‚Äô ‚Äòec2-user‚Äô etc.Root CauseWhen EC2 reboots, temp files in the system‚Äôs temp folder may be cleaned, which leads to the HDFS cannot startup correctly.Solution  create a new temp folder (supposing /home/ubuntu/apps/hadoop/tmp)  add the hadoop.tmp.dir property in the core-site.xml file (located under etc/hadoop in the $HADOOP_ROOT)  format the HDFS  restart hadoopNote: The  section will look like the following after the fix.&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;fs.defaultFS&lt;/name&gt;\t\t&lt;value&gt;hdfs://localhost:9000&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;      \t&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;      \t&lt;value&gt;/home/ubuntu/apps/hadoop/tmp&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;","categories": ["big-data"],
        "tags": ["hadoop"],
        "url": "http://localhost:4000/big-data/hadoop-hdfs-connection/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "BigQuery using Python and SQL",
        "excerpt":"Here is the basic steps to use SQL and Python in BigQuery.Step 1:# import package with helper functionsimport bq_helper# create a helper object for this datasetaccidents = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",                                   dataset_name=\"nhtsa_traffic_fatalities\")Step 2:-- query to find out the number of accidents which-- happen on each day of the weekquery = \"\"\"SELECT COUNT(consecutive_number),                  EXTRACT(DAYOFWEEK FROM timestamp_of_crash)            FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`            GROUP BY EXTRACT(DAYOFWEEK FROM timestamp_of_crash)            ORDER BY COUNT(consecutive_number) DESC        \"\"\"Step 3:# the query_to_pandas_safe method will cancel the query if# it would use too much of your quota, with the limit set# to 1 GB by defaultaccidents_by_day = accidents.query_to_pandas_safe(query)# accidents_by_day = accidents.query_to_pandas_safe(query, max_gb_scanned=6) # 6G MemoryStep extra:# library for plottingimport matplotlib.pyplot as plt# make a plot to show that our data is, actually, sorted:plt.plot(accidents_by_day.f0_)plt.title(\"Number of Accidents by Rank of Day \\n (Most to least dangerous)\")Note: The source code comes from my SQL Scavenger Hunt: Day 3","categories": ["big-data"],
        "tags": ["BigQuery"],
        "url": "http://localhost:4000/big-data/bigquery-sql-python/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Big Data Certificate @Bittiger!",
        "excerpt":"2018-02-19, Seattle","categories": ["learn"],
        "tags": ["certificate"],
        "url": "http://localhost:4000/learn/bittiger-bigdata-certificate/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"},{
        "title": "Use MapReduce to Find the Top k Rated Movies",
        "excerpt":"Use MapReduce to find the top k rated movies in the movieLens datasetDownload the following dataset and copy all the files to a folder in HDFSMovieLens 10M - Stable benchmark dataset. 10 million ratings and 100,000 tag applications applied to 10,000 movies by 72,000 users.http://grouplens.org/datasets/movielensWrite a MapReduce to find the top 25 rated movies in the movieLens dataset.ASSUMPTIONS  The term ‚Äútop 25 rated movies‚Äù means the first 25 movies sorted by the number of ratings for each movie.  If there is a tie in the last movie, all the movies will be counted except the last movie in the top list.  If there is a tie in the last movie of the top list, all the movies with the same number of rating will show.For example:Taking top 5 rated movies for example:  Input: A:5, B:4, C:4, D:3, E:3, F:3, G:2Output:A:5B:4C:4D:3E:3F:3Note: The format of the rating is MovieID:counts of ratingExplanation:  Movie A has the highest number of rates: 5. So now 4 to go.  Movie B and C are ties, so now 2 to go because they are counted 2 movies.  Movie D is included because its number of the rating is 3. Now 1 seat is left.  Movie E and F are included since they are ties with movie D.  Movie G is not included since there are actually 6 movies in the top list now.  So the output will display 6 movies instead of 5 due to the tie in the last movie.ANALYSIS AND DESIGN  Count the number of rating for each movie using one MapReduce job.All ratings are contained in the file ‚Äúratings.dat‚Äù and are in the following format:UserID::MovieID::Rating::Timestamp- UserIDs range between 1 and 6040- MovieIDs range between 1 and 3952- Ratings are made on a 5-star scale (whole-star ratings only)- Timestamp is represented in seconds since the epoch as returned by time(2)- Each user has at least 20 ratingsSo, I can use MovieID as the key and 1 as the value in the Hadoop job.Optimization: Use combiner to count the number of rating to light the workload of reducing.  Use another Hadoop job to sort and select the top 25 MovieID.The output of step 1 will be ‚ÄúMovieID  Count of Ratings‚Äù. So step two to take it as input and use the Count of Ratings as the key instead to sort the count of ratings in descending order.Then, the program will select from the top ratings and count how many in the top list (25).  if the selected movie numbers in the top list is less than 25, continue  select the next movie or all the movies if there is a tie  update the counter of the top list  continue from the beginning  Get the movie name information from the movies.dat using MovieIDs from step 2.Movie information is in the file ‚Äúmovies.dat‚Äù and is in the following format: MovieID::Title::GenresSo, joining the MoviedID from step 2 with this file to get the movie name information. This could be done by using a movie hash map [movieid:name] in a Java program. However, I will just manually search and get the result for simplicity.Notes  Step 3 will combine with step 2.  I will generalize this problem as ‚ÄòTop K‚Äô by taking a parameter in step 2 in the implementation.JOB 1: COUNT THE NUMBER OF RATINGSSOURCE CODEimport java.io.IOException;import java.util.stream.StreamSupport;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class RatingCounter extends Configured implements Tool {    private static final IntWritable ONE = new IntWritable(1);    public static class CountMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; {        private Text word = new Text();        @Override        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {            word.set(value.toString().split(\"::\")[1]); // change delimiter and index here            context.write(word, ONE);        }    }    public static class CountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {        @Override        public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)                throws IOException, InterruptedException {            int counter = StreamSupport.stream(values.spliterator(), true).mapToInt(IntWritable::get).sum();            // the second param of StreamSupport.stream() determines whether it is a parallel Stream.            context.write(key, new IntWritable(counter));        }    }    @Override    public int run(String[] args) throws Exception {        Configuration conf = this.getConf();        Job job = Job.getInstance(conf, \"Counter\");        job.setJarByClass(RatingCounter.class);        FileInputFormat.addInputPath(job, new Path(args[0]));        job.setInputFormatClass(TextInputFormat.class);        job.setMapperClass(CountMapper.class);        job.setCombinerClass(CountReducer.class);        job.setReducerClass(CountReducer.class);        FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(IntWritable.class);        return job.waitForCompletion(true) ? 0 : 1;    }    public static void main(String[] args) {        int code = -1;        try {            code = ToolRunner.run(new Configuration(), new RatingCounter(), args);        } catch (Exception e) {            e.printStackTrace();        }        System.exit(code);    }}InputPut the ‚Äòratings.dat‚Äô file to HDFS folder /movielens/ml-1m/rating/input.Execution  Compile: (Eclipse will automatically compile and output the classes files in $PROJECT_ROOT/bin)  Package: jar cf rc.ar RatingCounter*.class  Run: hadoop jar rc.jar RatingCounter /movielens/ml-1m/rating/input /movielens/ml-1m/rating/outputOUTPUTThe first 10 lines of the output are as below:1    207710    888100    1281000    201002    81003    1211004    1011005    1421006    781007    232Please refer to the ratingcounter.dms for all the contents.JOB 2: GET THE TOP KSOURCE CODE//filename: TopK.javaimport java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.Text;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;import sun.security.krb5.Config;public class TopK extends Configured implements Tool {    private static int k = 25;//dynamic k: hadoop jar topk.jar TopK -D topk=10 /input_dir /output_dir    public static class RankMapper extends Mapper&lt;Text, Text, LongWritable, Text&gt; {        @Override        public void map(Text item, Text vote, Context context) throws IOException, InterruptedException {            context.write(new LongWritable(Long.parseLong(vote.toString())), item);        }    }    public static class RankReducer extends Reducer&lt;LongWritable, Text, Text, LongWritable&gt; { //bugfix        private int counter = 0;                @Override        protected void setup(Context context) throws IOException, InterruptedException {            k = context.getConfiguration().getInt(\"topk\", k);            counter = 0;        }        @Override        public void reduce(LongWritable vote, Iterable&lt;Text&gt; items, Context context)                throws IOException, InterruptedException {            if (counter &lt; k) { // display the ties at last item                for (Text item : items) {                    context.write(item, vote); // flip the key and value                    counter++;                }            }        }    }    @Override    public int run(String[] args) throws Exception {        Configuration conf = this.getConf();        Job job = Job.getInstance(conf, \"Top K Board\");        job.setJarByClass(TopK.class);        job.setMapperClass(RankMapper.class);        job.setInputFormatClass(KeyValueTextInputFormat.class);        job.setMapOutputKeyClass(LongWritable.class);        job.setMapOutputValueClass(Text.class);        job.setReducerClass(RankReducer.class);        job.setOutputKeyClass(Text.class);        job.setOutputValueClass(LongWritable.class);        job.setNumReduceTasks(1);        job.setSortComparatorClass(LongWritable.DecreasingComparator.class);                FileInputFormat.addInputPath(job, new Path(args[0]));        FileOutputFormat.setOutputPath(job, new Path(args[1]));        job.submit();        return job.waitForCompletion(true) ? 0 : 1;    }    public static void main(String[] args) {        System.out.println(\"Get the top \" + k +\". You can set the k value: hadoop jar topk.jar TopK -D topk=10 /input_dir /output_dir\");        int code = -1;        try {            code = ToolRunner.run(new Configuration(), new TopK(), args);        } catch (Exception e) {            e.printStackTrace();        }        System.exit(code);    }}INPUTThe input would be the output of the first map reduce job file ‚Äòratingcounter.dms‚Äô.EXECUTIONjar cf topk.jar TopK*.classhadoop jar topk.jar TopK /movielens/ml-1m/rating/output /movielens/ml-1m/rating/topkNote: To keep the input directory clean, the needless files ‚Äò_SUCCESS‚Äô need to be deleted.OUTPUTHere is the output of this map reduce job.2858    3428260    29911196    29901210    2883480    26722028    2653589    26492571    25901270    2583593    25781580    25381198    2514608    25132762    2459110    24432396    23691197    2318527    23041617    22881265    22781097    22692628    22502997    2241318    2227858    2223Note: The first column is the MovieID, and the second one is the number of ratings.TOP 25 MOST RATED MOVIESHere comes the list in ratings descending order.2858::American Beauty (1999)::Comedy|Drama260::Star Wars: Episode IV - A New Hope (1977)::Action|Adventure|Fantasy|Sci-Fi1196::Star Wars: Episode V - The Empire Strikes Back (1980)::Action|Adventure|Drama|Sci-Fi|War1210::Star Wars: Episode VI - Return of the Jedi (1983)::Action|Adventure|Romance|Sci-Fi|War480::Jurassic Park (1993)::Action|Adventure|Sci-Fi2028::Saving Private Ryan (1998)::Action|Drama|War589::Terminator 2: Judgment Day (1991)::Action|Sci-Fi|Thriller2571::Matrix, The (1999)::Action|Sci-Fi|Thriller1270::Back to the Future (1985)::Comedy|Sci-Fi593::Silence of the Lambs, The (1991)::Drama|Thriller1580::Men in Black (1997)::Action|Adventure|Comedy|Sci-Fi1198::Raiders of the Lost Ark (1981)::Action|Adventure608::Fargo (1996)::Crime|Drama|Thriller2762::Sixth Sense, The (1999)::Thriller110::Braveheart (1995)::Action|Drama|War2396::Shakespeare in Love (1998)::Comedy|Romance1197::Princess Bride, The (1987)::Action|Adventure|Comedy|Romance527::Schindler's List (1993)::Drama|War1617::L.A. Confidential (1997)::Crime|Film-Noir|Mystery|Thriller1265::Groundhog Day (1993)::Comedy|Romance1097::E.T. the Extra-Terrestrial (1982)::Children's|Drama|Fantasy|Sci-Fi2628::Star Wars: Episode I - The Phantom Menace (1999)::Action|Adventure|Fantasy|Sci-Fi2997::Being John Malkovich (1999)::Comedy318::Shawshank Redemption, The (1994)::Drama858::Godfather, The (1972)::Action|Crime|DramaFURTHERMOREThis is out of the assignment‚Äôs scope but what could be done as follow-ups are:      [x] generalize the K so that this program could be used to count the top K    Set the parameter of this job‚Äôs configuration. For example, run this command to get the top 10 rated movies.    hadoop jar topk.jar TopK -D topk=10 /input_dir /output_dir        implement the [movieid:name] hash map so it can display the movie name instead of id automatically  ","categories": ["big-data"],
        "tags": ["hadoop"],
        "url": "http://localhost:4000/big-data/top-k-rated-movies/",
        "teaser":"http://localhost:4000/assets/images/default-teaser.png"}]
